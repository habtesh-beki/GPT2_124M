{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "class GPT_Tokenizer:\n",
        "   def __init__(self):\n",
        "      self.enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "   def encode(self, text):\n",
        "       self.encoding = self.enc.encode(text)\n",
        "       return self.encoding\n",
        "\n",
        "   def decode(self, token):\n",
        "       self.decoding = self.enc.decode(token)\n",
        "       return self.decoding"
      ],
      "metadata": {
        "id": "soeLFqZUa3UQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-2 tokenizer implementation using OpenAI's tiktoken library.\n",
        "Provides encoding and decoding functionality compatible with OpenAI's GPT models."
      ],
      "metadata": {
        "id": "2nx4EsIj7BEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"iLoveMerge.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "   raw_data = file.read()"
      ],
      "metadata": {
        "id": "SO9-5uX3B7IR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the text file containing training data for tokenization and model training"
      ],
      "metadata": {
        "id": "oLZcbRTuOXsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = raw_data[:100]\n",
        "print(text_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NZVkvEyL1CK",
        "outputId": "65c064d7-8cfb-45f2-a658-a230cd94a597"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Welcome To The World of Free Plain Vanilla Electronic Texts**\n",
            "\n",
            "**Etexts Readable By Both Humans an\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For demonstration, using the first 100 characters which will be encoded later"
      ],
      "metadata": {
        "id": "kcIhoKWIOy6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT_Tokenizer()\n",
        "encode_text = tokenizer.encode(text_data)\n",
        "print(encode_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hj1Pp0o0L9mV",
        "outputId": "ea3235e5-64fb-41ce-907b-92b586e9c263"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1174, 14618, 1675, 383, 2159, 286, 3232, 28847, 33897, 19508, 8255, 82, 1174, 198, 198, 1174, 36, 5239, 82, 4149, 540, 2750, 5747, 27411, 281]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization complete: words converted to tokens using tiktoken"
      ],
      "metadata": {
        "id": "9_XdpDn8PEX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decode_text = tokenizer.decode(encode_text)\n",
        "print(decode_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2hN8vOeNdmc",
        "outputId": "f02b7ca0-3052-4480-d9c5-d9f9bc1e69ff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Welcome To The World of Free Plain Vanilla Electronic Texts**\n",
            "\n",
            "**Etexts Readable By Both Humans an\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoding reproduces the exact original text that was encoded"
      ],
      "metadata": {
        "id": "6O_Lw8A3PW_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepares the dataset for LLM training using PyTorch's DataLoader.\n",
        "\n",
        "### This structures the data into batches, enabling efficient GPU utilization\n",
        "### and shuffling. For next-token prediction (causal LM), the target sequence\n",
        "### is the input sequence shifted right by one position."
      ],
      "metadata": {
        "id": "uHVUrvaWPC8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Implement_Dataset(Dataset):\n",
        "    def __init__(self, text, max_length, stride):\n",
        "        self.input_id = []\n",
        "        self.target_id = []\n",
        "\n",
        "        tokenizer = GPT_Tokenizer()\n",
        "        token_id = tokenizer.encode(text)\n",
        "\n",
        "        # range should use step=stride, not a second argument\n",
        "        for i in range(0, len(token_id) - max_length, stride):\n",
        "            input_seq = token_id[i : i + max_length]\n",
        "            target_seq = token_id[i + 1 : i + max_length + 1]\n",
        "\n",
        "            self.input_id.append(input_seq)\n",
        "            self.target_id.append(target_seq)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_id)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.input_id[idx]), torch.tensor(self.target_id[idx])\n"
      ],
      "metadata": {
        "id": "iY8qzyYLPshp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code implements a custom dataset class for our LLM training. Next, we’ll create a DataLoader to fetch the data in batches and iterate over it during training."
      ],
      "metadata": {
        "id": "_g3oE2k4WTBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Implement_DataLoader(txt,batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True,num_workers=0):\n",
        "      dataset = Implement_Dataset(txt, max_length, stride)\n",
        "      dataloader = DataLoader(dataset,\n",
        "                              batch_size= batch_size ,\n",
        "                              shuffle= shuffle,\n",
        "                              drop_last= drop_last ,\n",
        "                              num_workers=num_workers)\n",
        "      return dataloader"
      ],
      "metadata": {
        "id": "k8e_sKqDWqFh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = Implement_DataLoader(text_data, batch_size=4, max_length=8, stride=4)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "# first_batch = next(data_iter)\n",
        "# print(first_batch)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjMaDUYZhqeD",
        "outputId": "2885edf9-468b-4dad-e46e-4c63bc9722c0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " tensor([[33897, 19508,  8255,    82,  1174,   198,   198,  1174],\n",
            "        [   36,  5239,    82,  4149,   540,  2750,  5747, 27411],\n",
            "        [ 1174, 14618,  1675,   383,  2159,   286,  3232, 28847],\n",
            "        [ 1174,   198,   198,  1174,    36,  5239,    82,  4149]])\n",
            "\n",
            "Targets:\n",
            " tensor([[19508,  8255,    82,  1174,   198,   198,  1174,    36],\n",
            "        [ 5239,    82,  4149,   540,  2750,  5747, 27411,   281],\n",
            "        [14618,  1675,   383,  2159,   286,  3232, 28847, 33897],\n",
            "        [  198,   198,  1174,    36,  5239,    82,  4149,   540]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing Embeddings\n",
        "### Combines token embeddings with positional encodings to give the model\n",
        "### information about both word identity and sequence position."
      ],
      "metadata": {
        "id": "F4P0aUXa7eP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "      def __init__(self, emb_dim, vocab_size, max_length):\n",
        "        self.token_emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.pos_emb = nn.Embedding(max_length, emb_dim)\n",
        "\n",
        "      def forward(self, x):\n",
        "          positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n",
        "          return self.token_emb(x) + self.pos_emb(positions)"
      ],
      "metadata": {
        "id": "rO8SJdtEscTB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The core of GPT is the attention mechanism inside the Transformer. In the code below, I implement it step by step: first by initializing the query, key, and value projection weights, then by adding causal attention to prevent tokens from looking ahead. I also include dropout to improve generalization and reduce overfitting."
      ],
      "metadata": {
        "id": "AZhUb005ZwTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # Ensure even split across heads\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "\n",
        "        # Linear projections for Q, K, V\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "        # Final output projection\n",
        "        self.out_proj = nn.Linear(d_out, d_out)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Causal mask (upper triangular = 1 -> masked)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, seq_len, _ = x.shape\n",
        "\n",
        "        # Compute Q, K, V\n",
        "        Q = self.W_query(x)\n",
        "        K = self.W_key(x)\n",
        "        V = self.W_value(x)\n",
        "\n",
        "        # Reshape for multi-head: (b, seq_len, num_heads, head_dim)\n",
        "        Q = Q.view(b, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(b, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(b, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Attention scores: (b, num_heads, seq_len, seq_len)\n",
        "        scores = Q @ K.transpose(2, 3)\n",
        "\n",
        "        # Causal masking\n",
        "        mask = self.mask.bool()[:seq_len, :seq_len]\n",
        "        scores = scores.masked_fill(mask, float(\"-inf\"))\n",
        "\n",
        "        # Softmax over last dimension\n",
        "        attn = torch.softmax(scores / (self.head_dim ** 0.5), dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        # Weighted sum of values\n",
        "        context = attn @ V   # (b, num_heads, seq_len, head_dim)\n",
        "\n",
        "        # Merge heads back: (b, seq_len, d_out)\n",
        "        context = context.transpose(1, 2).contiguous().view(b, seq_len, self.d_out)\n",
        "\n",
        "        return self.out_proj(context)\n"
      ],
      "metadata": {
        "id": "Ew1qiX0dM0Wd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.manual_seed(123)\n",
        "\n",
        "\n",
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89, 0.55, 0.87, 0.66],  # Row 1\n",
        "     [0.57, 0.85, 0.64, 0.22, 0.58, 0.33],  # Row 2\n",
        "     [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]]  # Row 3\n",
        ")\n",
        "\n",
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(batch.shape)\n",
        "\n",
        "batch_size, context_length, d_in = batch.shape\n",
        "d_out = 6\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "context_vecs = mha(batch)\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAYMNUVp3eGK",
        "outputId": "db48a553-f517-4bc1-cc1b-4e426eaff3d4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 6])\n",
            "tensor([[[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
            "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
            "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]],\n",
            "\n",
            "        [[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
            "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
            "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 3, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_demo = \"Hello Every one\"\n",
        "tokenizer = GPT_Tokenizer()\n",
        "tokenized = tokenizer.encode(text_demo)\n",
        "print(tokenized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qptwZO0E8clI",
        "outputId": "cedfe773-e809-46d7-941e-dbc8747e0f2f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 3887, 530]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 1024, # Context length\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}"
      ],
      "metadata": {
        "id": "EgrOV2fCETBf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's implement layer normalization"
      ],
      "metadata": {
        "id": "I9QRsmse3GxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n"
      ],
      "metadata": {
        "id": "TzU_WkD-3Lh5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "batch_example = torch.randn(2, 5) #A"
      ],
      "metadata": {
        "id": "DwXbGmJ_Hrp-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(batch_example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_db-DXSKH-Ob",
        "outputId": "200dfa06-00da-4c7b-cafc-368b2a2fe641"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.6669,  0.5074, -1.1026, -0.3533, -1.7799],\n",
            "        [ 0.6474,  0.5460,  0.8050, -1.3467, -0.6418]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "ln = LayerNorm(emb_dim=5)\n",
        "out_ln = ln(batch_example)\n",
        "mean = out_ln.mean(dim=-1, keepdim=True)\n",
        "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
        "\n",
        "print(\"Mean:\\n\", mean)\n",
        "print(\"Variance:\\n\", var)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKSvzSyBIEaZ",
        "outputId": "06cb948c-d035-41ff-9ee5-c78b32e092fd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean:\n",
            " tensor([[-5.9605e-09],\n",
            "        [-4.7684e-08]], grad_fn=<MeanBackward1>)\n",
            "Variance:\n",
            " tensor([[1.0000],\n",
            "        [1.0000]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))"
      ],
      "metadata": {
        "id": "s05Lqo_Kz973"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n",
        "            GELU(), ## Activation\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "cOvD-7ary302"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        # 2*4*768\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x\n",
        "        # 2*4*768"
      ],
      "metadata": {
        "id": "_EJuI4dd0XJw"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "I-4F81bw1mIV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "tokenizer = GPT_Tokenizer()\n",
        "batch = []\n",
        "txt1 = \"Your hard work leads to\"\n",
        "txt2 = \"hard work is a key\"\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "batch = torch.stack(batch, dim=0)\n",
        "print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSiKuO9j4ewV",
        "outputId": "edbe4986-2db9-4f16-c7f5-619b01109125"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 7120,  1327,   670,  5983,   284],\n",
            "        [10424,   670,   318,   257,  1994]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "out = model(batch)\n",
        "print(\"Input batch:\\n\", batch)\n",
        "print(\"\\nOutput shape:\", out.shape)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hX2lBQ6f1rZo",
        "outputId": "68e09bc9-d2cd-49f3-8466-9c807fe41e0a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch:\n",
            " tensor([[ 7120,  1327,   670,  5983,   284],\n",
            "        [10424,   670,   318,   257,  1994]])\n",
            "\n",
            "Output shape: torch.Size([2, 5, 50257])\n",
            "tensor([[[ 0.8715,  0.7359, -0.4033,  ...,  0.3783,  0.0976, -1.1895],\n",
            "         [ 0.6399, -0.3078,  0.5744,  ...,  0.2013,  0.4935, -0.1959],\n",
            "         [ 1.1128,  0.4172, -0.7705,  ...,  0.4772, -0.7034, -0.9059],\n",
            "         [-0.2190,  0.1247,  0.4820,  ...,  0.6499, -0.5742, -0.4945],\n",
            "         [ 0.8533, -0.3853, -0.5923,  ..., -0.6437,  0.0973, -0.7483]],\n",
            "\n",
            "        [[-0.3444, -0.3724,  0.0283,  ..., -0.1598,  0.0271,  0.0101],\n",
            "         [ 0.0273, -0.5396, -1.3966,  ..., -0.1147, -0.5108, -0.5886],\n",
            "         [ 1.3964, -0.7803, -0.2001,  ...,  0.5523,  0.2406, -0.1905],\n",
            "         [-0.3339,  0.0140,  0.4530,  ...,  1.0095, -0.5696, -0.1425],\n",
            "         [ 0.2232, -0.0840, -0.0050,  ...,  0.2683,  0.1999,  0.4608]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (batch, n_tokens) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # then only the last context_size tokens are used as context 10 token 5\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        logits = logits[:, -1, :]\n",
        "        # Apply softmax to get probabilities\n",
        "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
        "\n",
        "        # Get the idx of the vocab entry with the highest probability value\n",
        "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "MStAYQ5BFrrN"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_context = \"Hello,I'm a developer\"\n",
        "encoded = tokenizer.encode(start_context)\n",
        "print(\"encoded:\", encoded)\n",
        "encoded_tensor = torch.tensor(encoded).unsqueeze(0) #A\n",
        "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TU1LqcLJFtnR",
        "outputId": "d873ffb3-431f-43d1-9b95-3247624178c8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoded: [15496, 11, 40, 1101, 257, 8517]\n",
            "encoded_tensor.shape: torch.Size([1, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval() #A\n",
        "#model = GPTModel(GPT_CONFIG_124M)\n",
        "out = generate_text_simple(\n",
        "model=model,\n",
        "idx=encoded_tensor,\n",
        "max_new_tokens=10,\n",
        "context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "print(\"Output:\", out)\n",
        "print(\"Output length:\", len(out[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQWHXx5BFyiN",
        "outputId": "29641f0d-7fc0-48c7-da1e-9c3eca84c900"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: tensor([[15496,    11,    40,  1101,   257,  8517,  3271, 46935, 17060, 39258,\n",
            "           297, 49965,  7385, 38855,  2046,  4344]])\n",
            "Output length: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXaMQo18GEU2",
        "outputId": "65426264-2345-4b01-cbf2-b20535a5e6df"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello,I'm a developer David vaPutinれll GAM KirAccept fire adop\n"
          ]
        }
      ]
    }
  ]
}